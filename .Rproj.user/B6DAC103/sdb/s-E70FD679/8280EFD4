{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Introduction to R\"\nauthor: \"Zhaohu(Jonathan) Fan\"\noutput:\n  html_document:\n    fig_height: 7\n    fig_width: 9\n    keep_md: yes\n    toc: yes\n    toc_float: yes\n---\n\n```{r include=FALSE}\nlibrary(DataComputing)\n```\n\n```{r setup, include=FALSE}\nlibrary(knitr)\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n\n```\n# Activity 1: Stocks & Dividends\n### Getting Price Data\n\n```{r}\n# choose companies of interest\ncompanies <- c(\"F\", \"MMM\", \"GE\")\n\nPrices <- read_stock_prices(companies, what = \"daily\", start_year = 2000, end_year = 2016)\n\n# inspect data\nhead(Prices)\n\n# chart close vs date for each company\nPrices %>%\n  ggplot(aes(x = date, y = close)) + \n  geom_line(aes(color = company))\n\n```\n\n\n### Buy/Sell Profit\n\n\n```{r}\n\nActions <- \n  data.frame(\n    action = c(\"buy\", \"sell\"), \n    date = ymd(c(\"2008-07-14\", \"2015-12-16\"))\n  )\n\n# Combine the Prices & Actions tables\nSalesDifference <- \n  Prices %>%\n  inner_join(Actions) %>%\n  select(company, action, close) %>%\n  spread(key = action, value = close) %>%\n  mutate(profit = sell - buy)\n\n# inspect the data table\nSalesDifference\n\n```\n\n\n### Indexing Prices\n\n```{r}\n# choose reference date\nref_date <- ymd(\"2008-07-14\")\n\n# establish reference value for indexing\nReference <- \n  Prices %>%\n  filter(date == ref_date) %>%\n  select(company, standard=close)  # renames close variable as \"standard\"\n\n# inspect the result\nReference\n\n# index against reference value\nIndexData <- \n  Prices %>%\n  left_join(Reference) %>%\n  transmute(company, date, index = close/standard)\n\n# inspect the result\nhead(IndexData)\n\n# graph the indexes\nIndexData %>%\n  ggplot(aes(x = date, y = index)) + \n  geom_line(aes(color = company))\n```\n\n\n### Dividends\n\n Note that some companies don't issue dividends, so it must be removed or replaced before continuing.\n\n```{r eval=FALSE}\n# read dividend data\nDividends <- read_stock_prices(companies, what = \"dividends\")\n\n# inspect the data\nhead(Dividends)\n\nDividendsEarned <- \n  Prices %>%\n  inner_join(Dividends) %>%\n  filter(ymd(date) <= ymd(\"2015-12-16\"), ymd(date) >= ymd(\"2008-07-14\")) %>%\n  select(company, dividends) %>%\n  group_by(company) %>%\n  summarise(totalDividends = sum(dividends))\n\n# inspect the result\nhead(DividendsEarned)\n\n```\n\n```{r eval=FALSE}\n# earnings comparison\nSalesDifference %>%\n  inner_join(DividendsEarned)\n```\n\n\n# Activity 2: Bicycle Sharing\n### Set Up\n\n```{r}\n# Load the data sets into our RStudio environment as described in the text\nStations <- mosaic::read.file(\"http://tiny.cc/dcf/DC-Stations.csv\")\n\n# data_site <- \"http://tiny.cc/dcf/2014-Q4-Trips-History-Data-Small.rds\"  # small data with 10k rows\ndata_site <- \"http://tiny.cc/dcf/2014-Q4-Trips-History-Data.rds\"        # full data with 600k rows\n\nTrips <- readRDS(gzcon(url(data_site)))\n\n# Inspect the data tables\nstr(Stations)\nstr(Trips)\n\n# packages loaded for A.5 leaflet graphic\nlibrary(devtools)\nlibrary(leaflet)\n```\n\n##### Check out times.\n\nThe following plot uses the POSIXct data type associated with check-out times (variable: `sdate`).\n\n```{r}\nTrips %>%\n  ggplot(aes(x = sdate)) + \n  geom_density(fill = \"gray\", color = NA)\n```\n\n\n### A.1 How Long?\n\nThe following box & whisker plot shows the distribution of rental duration by client type with outliers removed.\n\n```{r}\n# select the variables of use for the activity & create a duration variable\nTrips <- \n  Trips %>%\n  mutate(durMin = as.numeric(edate - sdate)/60)  # trip duration in minutes\n\n# inspect data table; discern units of \"durMinutes\"\nhead(Trips)\n\n# boxplot\nTrips %>%\n  ggplot(aes(x = client, y = durMin)) + \n  geom_boxplot() + \n  ylim(0, 90) +           # restrict plot to 90 minutes or less\n  ylab(\"Rental Duration (min)\") + \n  xlab(\"Client Type\")\n```\n\n\n###  A.2 When are bikes used?\n\nExplore bike use for the following: \n\n* day of the year (1 to 365)\n* day of the week (Sunday to Saturday)\n* hour of the day (0 to 24)\n* minute in the hour (0 to 60)\n\nWe first need to create these variables in the `Trips` data table using a `mutate()` statement.\n\n```{r}\nTrips <- \n  Trips %>%\n  mutate(dayOfYear = lubridate::yday(sdate), \n         dayOfWeek = lubridate::wday(sdate), \n         dayOfWeekLabel = lubridate::wday(sdate, label = TRUE), \n         hourOfDay = lubridate::hour(sdate), \n         minuteOfHour = lubridate::minute(sdate))\n\n# head(Trips)     # Inspect data table (commented out for now)\n```\n\n##### Day of the year (1 to 365)\n\nThe data suggest that usage declines toward the end of the year.  (Note: the data set is said to include \"rental history over the last quarter of 2014\" so there is no information in this data set for January through September)\n```{r}\nTrips %>%\n  ggplot(aes(x = dayOfYear)) + \n  geom_density(fill = \"gray\", adjust = 2)\n```\n\n##### Day of the week (Sunday to Saturday)\n\nWe see usage is quite consistent across the weekdays, and then a bit reduced on weekends.\n\n```{r}\nTrips %>%\n  ggplot(aes(x = dayOfWeek)) + \n  geom_density(fill = \"gray\", adjust = 2)\n```\n\nDensity isn't wrong, but it's a little goofy here.  Actually, it's pretty easy to turn the day of week from numeric to the names as we know them with `dayOfWeekLabel = lubridate::wday(sdate, label = TRUE)` so let's do that and make it a bar chart to see how that looks.  \n\n```{r}\nTrips %>%\n  ggplot(aes(x = dayOfWeekLabel)) + \n  geom_bar(fill = \"gray\") \n```\n\n\n\n##### Hour of the day (0 to 24)\n\nFew bicycles are checked out before 5am, and then we see usage spike near 8am and 5pm in concert with rush hour commuting.\n\n```{r}\nTrips %>%\n  ggplot(aes(x = hourOfDay)) + \n  geom_density(fill = \"gray\", adjust = 2)\n```\n\n##### Minute in the hour (0 to 60)\n\nUsage appears to drop near the top of the hour. \n\n```{r}\nTrips %>%\n  ggplot(aes(x = minuteOfHour)) + \n  geom_density(fill = \"gray\", adjust = 2)\n```\n\n\n##### Group the bike rentals by hour, weekday, & client type\n\nWe can see that the rush hour spikes (8am & 5pm) are much more pronounced among registered users on weekdays.\n\n```{r}\nTrips %>%\n  group_by(client, dayOfWeek, hourOfDay) %>%\n  summarise(count = n()) %>%\n  ggplot(aes(x = hourOfDay, y = count)) + \n  geom_line(aes(group = dayOfWeek, color = as.character(dayOfWeek))) + \n  facet_wrap( ~ client)\n```\n\n\n##### Construct a similar plot using density rather than frequency\n\nThe density appears to present the information in a more \"relative\" than \"absolute\" way.  The density could be misinterpreted to suggest that bicycle usage among casual users is higher than registered users.\n\n```{r}\nTrips %>%\n  ggplot(aes(x = hourOfDay)) + \n  geom_density(aes(group = dayOfWeek, color = as.character(dayOfWeek))) + \n  facet_wrap( ~ client)\n```\n\n\n###  A.3 How Far? \n\nWe want to calculate the distance between stations using the `haversine()` function.\n\n```{r}\nsource(\"http://tiny.cc/dcf/haversine.R\")\n```\n\nWe want to create a table where a case is a *pair* of stations.  We first need to create tables that represent the starting station and another table that represents the ending station in each pair.  Since we ultimately want to join this information with the `Trips` data, we use the variable names `sstation` and `estation` since those represent the starting and ending stations we will want to match from that data table.\n\n```{r}\n# inspect the Stations data\nhead(Stations)\n\n# create a working copy of the data & rename to identify starting Station\nSimple <- \n  Stations %>%\n  rename(sstation = name)\n\n# create another copy and rename variables for use as the ending station\nSimple2 <- \n  Simple %>%\n  rename(estation = sstation, lat2 = lat, long2 = long)\n\n# inspect Simple & Simple2\nhead(Simple)\nhead(Simple2)\n```\n\n\nIn order to create pairs matching every single beginning station to every single ending station, we need a type of join called a Cartesian product of our startStation & endStation data tables.  The book explains that Cartesian products can be dangerous if the tables are too large, so you need to think before you do a join like this.  With 347 cases in each data table, the Cartesian product results in  $347 * 347 = 120409$ cases.  This is certainly a manageable size, so we'll proceed.\n\n```{r}\n# join the starting and ending data tables together\nStationPairs <- \n  merge(Simple, Simple2, by = NULL)\n\n# inspect station pairs data table\nhead(StationPairs)\n\n# compute distances with haversine()\nPairDistances <- \n  StationPairs %>%\n  transmute(sstation, estation, distance = haversine(lat, long, lat2, long2)) \n\n# inspect PairDistances\nhead(PairDistances)\n\n```\n\n```{r}\nPairDistances %>%\n  ggplot(aes(x = distance)) + \n  geom_density(fill = \"gray\")\n\n```\n\nThe book says that \"one end of Washington DC to the other is about 14.1 miles\" so we need to investigate a bit further since our density plot of station distances doesn't seem to reflect that.  We see below that the furthest stations are about 37.03506 (units) apart.  We might first suspect (correctly) that the \"units\" produced by the `haversine()` function are kilometers and not miles.  Still, 14.1 miles is less than 23 km, but our largest possible distance pair is 37.03506 km (23 miles) from \"Needwood Rd & Eagles Head Ct\" to \"Prince St & Union St\".  A quick investigation using [Google Maps](https://goo.gl/maps/c2gecqmEeaP2) reveals a walking distance of about 25 miles between those locations.  Walking distance can come close to the true distance \"as the crow flies\" but a few minor detours would be required in order to walk on paved paths, cross the Potomac River, etc.  It looks like the distances have been calculated correctly, but the \"hint\" in the text book was perhaps misleading since \"one end to the other\" isn't necessarily the diameter between the most extreme points (i.e. corner to corner).\n\n```{r}\nPairDistances %>%\n  arrange(desc(distance)) %>%\n  head()\n```\n\n\n\nNext, we want to join the `PairDistances` table with `Trips` to calculate the start-to-end distance of each trip. \n\n\n```{r}\nRideDistances <- \n  Trips %>%\n  inner_join(PairDistances, by = c(\"sstation\", \"estation\"))\n```\n\nThe distances of the actual rides tend to be only a few kilometers or less.  Most rides are between stations that are fewer than about 5 kilometers apart.  Of course, the ride itself may be longer, but the stations aren't very far apart in absolute distance.  Most of the station pairs are farther apart than 5 km, but people don't tend to use the bicycle share program to travel that far since other transportation would probably be much more efficient (and possibly safer?) as travel distance increases.  \n\nNote: for the purpose of the assignment, it's fine to just show the plot of ride distances without adding the layer to show distance between pairs of stations on the same plot.  \n\n```{r}\nRideDistances %>%\n  ggplot(aes(x = distance)) + \n  geom_density(fill = \"gray\") + \n  geom_density(data = PairDistances, aes(x = distance), fill = \"purple\", alpha = 0.1)\n```\n\n\n###  A.4 Mapping the Stations\n\n```{r}\nstationMap <- \n  leaflet(Stations) %>%\n  addTiles() %>%\n  addCircleMarkers(radius = 2, color = \"red\") %>%\n  setView(-77.04, 38.9, zoom = 12) \n\nstationMap\n```\n\n\n### A.5 Long-distance stations \n\nBased on the map, the median distance traveled from each station appears to be negatively correlated with station density.  In other words, when there are many stations in close proximity the median ride distance tends to be shorter by comparison to areas of the city where the ride stations are more sparse.\n\n```{r}\nStationPairMap <- \n  RideDistances %>%\n  rename(name = sstation) %>% # rename to facilitate join operation\n  left_join(Stations) %>%     # adds lat & long\n  transmute(name, lat, long, distProxy = distance * 1000/3) %>%\n  group_by(name, lat, long) %>%\n  summarise(medDist = median(distProxy))\n\nhead(StationPairMap)\n\ndistanceMap <-\n  leaflet(StationPairMap) %>%\n  addTiles() %>%\n  addCircleMarkers(radius = 2, color = \"red\") %>%\n  addCircles(radius = ~ medDist, color = \"blue\", opacity = 0.0001) %>%\n  setView(-77.04, 38.9, zoom = 12)\n\ndistanceMap\n\n```\n\n\nBased on the map, the median distance traveled from each station appears to be negatively correlated with station density.  In other words, when there are many stations in close proximity the median ride distance tends to be shorter by comparison to areas of the city where the ride stations are more sparse.  \n\nWith a small modification (i.e. `addCircleMarkers(radius = 2, color = \"red\", opacity = ~ freqUsage)`), we can darken the location marker of each bicycle share station to reflect the frequency of it's use.  With this information included, the plot below shows that near the city center rides tend to be shorter distances and the stations are utilized much more frequently.  By contrast, near the fringes of the city rides tend to be longer distances and the stations are utilized less frequently.\n\n```{r}\nStationPairMap <- \n  RideDistances %>%\n  rename(name = sstation) %>% # rename to facilitate join operation\n  left_join(Stations) %>%     # adds lat & long\n  transmute(name, lat, long, distProxy = distance * 1000/3) %>%\n  group_by(name, lat, long) %>%\n  summarise(medDist = median(distProxy), freqUsage = 0.0002 * n())\n\nfreqMap <-\n  leaflet(StationPairMap) %>%\n  addTiles() %>%\n  addCircleMarkers(radius = 2, color = \"red\", opacity = ~ freqUsage) %>%\n  addCircles(radius = ~ medDist, color = \"blue\", opacity = 0.0001) %>%\n  setView(-77.04, 38.9, zoom = 12)\n\nfreqMap\n\n```\n\n\n# Activity 3: Statistics of Gene Expression\n### Set-Up\n\n```{r}\ndata(\"NCI60\")\ndata(\"NCI60cells\")\n```\n\n\n### Simple Graphics for Gene Expression\n\n```{r}\n## Inspect raw data\n# head(NCI60)\n# head(NCI60cells)\n\n## Convert NCI60 to narrow & drop \"Probe\" variable\nNarrow <- \n  NCI60 %>%\n  tidyr::gather(cellLine, expression, -Probe)\n\n## Inspect results\n# head(Narrow)\n\n## select cellLine & tissue from NCI60cells data; the `gsub()` function is used to conform cellLine to match the NCI60 syntax\nCellTypes <- \n  NCI60cells %>%\n  select(cellLine, tissue) %>%\n  mutate(cellLine = gsub(\"\\\\:\", \".\", as.character(cellLine)))\n\n## Inspect results\n# head(CellTypes)\n\n## reduce data to cases with matching `cellLine` from both tables\nNarrow <- \n  Narrow %>%\n  inner_join(CellTypes)\n\n## Inspect result\nhead(Narrow)\n```\n\nWe can now extract the expression of TOP3A for each cell line and calculate the mean expression for each tissue type.\n\n```{r}\n## restrict data to include only the TOP3A probes\nProbe_TOP3A <- \n  Narrow %>% filter(Probe==\"TOP3A\")\n\n## calculate mean expression (note: `exp(x)` is syntax for e^x where \"e\" is Euler's number: approx. 2.718)\nSummaryStats <- \n  Probe_TOP3A %>%\n  group_by(tissue) %>%\n  summarise(mn_expr = exp(mean(expression, na.rm = TRUE)))\n\n## Inspect the result (reproduce Table A.8)\nkable(SummaryStats)\n\n## Bar chart of mean expression of TOP3A (Figure A.6)\nSummaryStats %>%\n  ggplot(aes(x = tissue, y = mn_expr)) + \n  geom_bar(stat = \"identity\") + \n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n## dot plot of mean expression of TOP3A (Figure A.7)\nProbe_TOP3A %>%\n  ggplot(aes(x = tissue, y = exp(expression))) + \n  geom_point() + \n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n```\n\n\n##### Critique of Figure A.6\n\nSome of the shortcomings of Figure A.6 might include:\n* bars alone may oversimplify the data leading to misinterpretation\n* may be useful to order the bars by height to facilitate tissue comparisons\n* precision of the estimate is not shown\n* too much ink\n\n##### Improving the graphic\n\n1. Lighten up the color using `alpha = 0.2` or perhaps switch to a dot plot (i.e. Figure A.7)\n\n2. Reorder the tissue types\n\n3. Show statistical measure of variation\n\n```{r}\n## Calculate mean, standard error, and confidence interval\nSummaryStats <-\n  Probe_TOP3A %>%\n  group_by(tissue) %>%\n  summarise(mn = mean(expression, na.rm = TRUE), \n            se = sd(expression, na.rm = TRUE) / sqrt( n() )) \n```\n\n\n4. Show the expression value for each of the individual cases in `MyProbe` (i.e. `TOP3A` here)\n\n5. Use different modality, e.g. dot plot, box plot (`notch = TRUE`), violin plot\n\n##### Reproduce Dot Plot with Confidence Intervals (Figure A.8)\n\n```{r}\nSummaryStats %>%\n  ggplot(aes(x = tissue, y = exp(mn))) + \n  geom_bar(stat = \"identity\", fill = \"gray\", color = NA) + \n  geom_point(data = Probe_TOP3A, aes(x = tissue, y = exp(expression))) + \n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n```\n\n##### Reproduce \"Dynamite Plot\" in Figure A.9\n\n```{r}\n## Calculate mean, standard error, and confidence interval\nSummaryStats <-\n  SummaryStats %>%\n  mutate(top = mn + 2 * se, \n         bottom = mn - 2 * se)\n\n\n## \"Dynamite Plot\"\nSummaryStats %>% \n  ggplot(aes(x = tissue, y = exp(mn))) + \n  geom_bar(stat = \"identity\", alpha = 0.2) + \n  geom_errorbar(aes(x = tissue, \n                    ymax = exp(top), \n                    ymin = exp(bottom)), width = 0.5) + \n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n  \n```\n\n\n##### Reproduce Dot Plot with Confidence Intervals (Figure A.10)\n\n```{r}\n\nSummaryStats %>% \n  ggplot(aes(x = tissue, y = exp(mn))) + \n  geom_errorbar(aes(x = tissue, \n                    ymax = exp(top), \n                    ymin = exp(bottom)), width = 0.5) + \n  geom_point(data = Probe_TOP3A, aes(x = tissue, y = exp(expression))) + \n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n```\n\n\n### Probing for a Probe\n\nCreate a function to calculate R-squared, and apply the function to each `Probe`. \n\n```{r}\n## Create function to calculate r-squared\nr2 <- function(data) {\n  mosaic::rsquared(lm(data$expression ~ data$tissue))\n}\n\n## apply function to each `Probe` (takes a while because there are a lot of probes)\nProbeR2 <- \n  Narrow %>%\n  group_by(Probe) %>%\n  do(r2 = r2(.)) %>%\n  mutate(r2 = unlist(r2))\n\n## Order the Probes by R2, and pull out the 30 largest\nActual <- \n  ProbeR2 %>%\n  arrange(desc(r2)) %>%\n  head(30) %>%\n  mutate(Probe = reorder(Probe, desc(r2)))\n\n## Show as a table\nkable(Actual)\n\n## Graph probes with top 30 r-squared\nActual %>%\n  ggplot(aes(x = Probe, y = r2)) + \n  geom_point() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n```\n\n\n### Choose a probe with high r-squared and create a plot like Figure A.10\n\nI chose to investigate probe PLK2 (for no particular reason).  A noteworthy difference between the plot associated with PLK2 and Figure A.10 based on TOP3A, is that there is a much more apparent association between certain tissue types and the probe.  Namely, the probe is much more common among renal tissue by compasion to the other tissue types studied.\n\n```{r}\n\n## I selected Probe PLK2, but any probe from the top 30 is fine.\nProbe_PLK2 <- \n  Narrow %>% filter(Probe==\"PLK2\")\n\n## Calculate mean, standard error, and confidence interval\nSummaryStats_PLK2 <-\n  Probe_PLK2 %>%\n  group_by(tissue) %>%\n  summarise(mn = mean(expression, na.rm = TRUE), \n            se = sd(expression, na.rm = TRUE) / sqrt( n() )) %>%\n  mutate(top = mn + 2 * se, \n         bottom = mn - 2 * se)\n\n## Reproduce Figure A.10 for Probe PLK2\nSummaryStats_PLK2 %>% \n  ggplot(aes(x = tissue, y = exp(mn))) + \n  geom_errorbar(aes(x = tissue, \n                    ymax = exp(top), \n                    ymin = exp(bottom)), width = 0.5) + \n  geom_point(data = Probe_PLK2, aes(x = tissue, y = exp(expression))) + \n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n```\n\n\n### False Discoveries\n\nGenerate a Null distribution in order to observe outcomes that might happen just by chance if in fact there were really no assocation at all between probe and tissue type.  We then compare our observed result to the Null distribution in order to determine whether there is a compelling difference between our result and the sort of outcomes we would expect to observe just by chance if the null hypothesis were true (i.e. if there were no association at all between probe & tissue type).\n\n```{r}\n## Build null distribution by breaking the association between probe and tissue\nNullR2 <- \n  Narrow %>%\n  group_by(Probe) %>%\n  mutate(expression = mosaic::shuffle(expression)) %>%\n  group_by(Probe) %>%\n  do(r2 = r2(.)) %>%\n  mutate(r2 = unlist(r2))\n\n##  Comparison of null distibution to the R2 observed for the actual data\nProbeR2 %>%\n  ggplot(aes(x = r2)) + \n  geom_density(fill = \"gray30\", color = NA) + \n  geom_density(data = NullR2, aes(x = r2), \n               fill = \"gray80\", alpha = 0.75, color = NA)\n\n\n```\n\n\nLastly, we compare the top 30 observed r-squared values with the top 30 values produced in the null distribution.  Since we generated the null distrubtion using random simulations (via `mosaic::shuffle()`), your results need not be identical to the picture in the book.  In fact they may appear slightly different each time you refresh the analysis!  Even still, the conclusion holds that none of the top 30 r-squared values for the actual data lie anywhere near those from the null hypothesis.\n\n```{r}\nNull <- \n  NullR2 %>%\n  arrange(desc(r2)) %>%\n  head(30)\n\nActual$null <- Null$r2\n\nActual %>%\n  ggplot(aes(x = Probe, y = r2)) + \n  geom_point() + \n  geom_point(aes(y = null), color = \"gray50\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n```\n\n\n# Activity 4: Scraping Nuclear Reactors\n### Set-Up\n\n```{r}\nlibrary(rvest)\nlibrary(lubridate)\n```\n\n\n### Nuclear Reactor Data\n\nThe Nuclear Reactor Data are scraped from Wikipedia: <https://en.wikipedia.org/wiki/List_of_nuclear_reactors>\n\n\n### Locate & Inspect Japan Reactor Data\n\nWhile scraping all tables on teh page, R reports a \"subscript out of bounds\" error.  We instead locate the Japan table directly, and then scrape it (this seems like the more direct way to scrape a single table anyway).\n\n```{r}\npage <- \"https://en.wikipedia.org/wiki/List_of_nuclear_reactors\"\nXPATH <- '//*[@id=\"mw-content-text\"]/table[23]'\n'//*[@id=\"mw-content-text\"]/table'\n\ntable_list <- \n  page %>%\n  read_html() %>%\n  html_nodes(xpath = XPATH) %>%\n  html_table(fill = TRUE)\n\n# extract the data table from the \"list\" (even though the list only has one data table in it)\nJapan <- table_list[[1]]\n\n# Inspect data table \nstr(Japan)\nhead(Japan)\n```\n\n### Data Cleaning\n\nIn what ways is the data table tidy?  \n\n* each column is a variable\n* rows are mostly cases (not quite tidy, but close)\n\nIn what ways is the data table NOT tidy?  \n\n* first row is a continuation of variable names (not a case)\n* DataComputing text alleges missing columns, but I didn't have that issue...\n\n##### Cleaning Up Variable Names\n\n```{r}\n# create unique names for variables 4 & 7\nnames(Japan)[c(4, 7)] <- c(\"model\", \"grossMW\")\n\n# Inspect result\nhead(Japan)\n\n# cleaning up the Japan Data\nJapan <- \n  Japan %>%\n  filter(row_number() > 1) %>%      # drop the first row in the table (continuation of names)\n  rename(name = Name, reactor = `Reactor No.`,    # Note the use of back-ticks for names with a space: `\n         type = Reactor, status = Status, \n         netMW = `Capacity in MW`, \n         construction = `Construction Start Date`, \n         operation = `Commercial Operation Date`, closure = Closure)\n\nhead(Japan)\n```\n\n##### Cleaning Up Variable Formats\n\nFrom `str()` we can see that everything is treated as character strings (even numeric and date variables).\n\n```{r}\nstr(Japan)\n```\n\nLet's convert each variable to the most appropriate type using `mutate()`.  Notice the warning \"Warning: 5 failed to parse\".  Can you spot from the original data table which 5 values may have caused the warning and why R may have failed to reformat them?  (Hint: look at the closure dates).  It's important to pay attention to warnings and understand where they come from!\n\n```{r}\nJapan <- \n  Japan %>%\n  mutate(netMW = as.numeric(netMW), grossMW = as.numeric(grossMW)) %>%   \n  mutate(construction = dmy(construction), operation = dmy(operation), closure = dmy(closure))  \n\n# Inspect Result\nhead(Japan)\n  \n```\n\n###Plot Net Generation Capacity vs Construction Date\n\nHere are a few possible observations from the graph:\n\n* most reactors are either type \"PWR\" or \"BWR\"\n* there is a slight positive trend indicating that perhaps reactors constructed more recently have greater net generation capacity\n* type \"PWR\" reactors may appear to be associated with slightly greater net generation capacity than \"BWR\" reactors constructed near the same time\n\n```{r}\nJapan %>% \n  ggplot(aes(y = netMW, x = construction)) + \n  geom_point(aes(color = type))\n```\n\n\n### Scrape & Merge China Data\n\n##### Scrape (& Clean)\n\nSince you're doing the same steps as before, you should use copy and paste liberally and just make small changes to adapt the code to the China data.  No need to re-type the whole thing!  You may get a few warnings again (at least I did), can you track down the problem cases in the original table?  \n\n```{r}\n\n# same webpage as before\npage <- \"https://en.wikipedia.org/wiki/List_of_nuclear_reactors\"\n\n# new xpath (we need the China table)\nXPATH <- '//*[@id=\"mw-content-text\"]/table[12]'\n\ntable_list <- \n  page %>%\n  read_html() %>%\n  html_nodes(xpath = XPATH) %>%\n  html_table(fill = TRUE)\n\n# extract the data table from the \"list\" (even though the list only has one data table in it)\nChina <- table_list[[1]]\n\n# Inspect data table \nstr(China)\nhead(China)\n\n\n# create unique names for variables 4 & 7\nnames(China)[c(4, 7)] <- c(\"model\", \"grossMW\")\n\n# cleaning up the China Data (almost identical to Japan cleaning)\nChina <- \n  China %>%\n  filter(row_number() > 1) %>%      \n  rename(name = Name, reactor = `Reactor No.`,\n         type = Reactor, status = Status, \n         netMW = `Capacity in MW`, \n         construction = `Construction Start Date`, \n         operation = `Commercial Operation Date`, closure = Closure) %>%\n  mutate(netMW = as.numeric(netMW), grossMW = as.numeric(grossMW)) %>%   \n  mutate(construction = dmy(construction), operation = dmy(operation))  \n\nhead(China)\n```\n\n\n##### Merging China and Japan\n\n```{r}\nJapan <- \n  Japan %>%\n  mutate(country = \"Japan\")\n\nChina <- \n  China %>%\n  mutate(country = \"China\")\n\n# Inspect the results\nhead(Japan)\nhead(China)\n\n# Combine them together\nChina_Japan <- rbind(China, Japan)\n\n# Inspect Results\nstr(China_Japan)   # structure\nhead(China_Japan)  # first rows\ntail(China_Japan)  # last rows\n```\n\n\n\n\n### Make Info Graphic of Japan Reactors\n\n```{r fig.height=12, fig.width=9}\nJapan %>%\n  mutate(name_reactor = paste(name, reactor)) %>%\n  ggplot(aes(y = name_reactor, x = operation, size = 4)) + \n  geom_segment(aes(y = name_reactor, yend = name_reactor, x = construction, xend = operation, color = type)) + \n  geom_point(aes(y = name_reactor, x = closure, shape = status))\n\n```\n\n\n\n# Activity 5: Bird Species\n\nWhen you're finished with this activity, you will have a graph that shows what time of year various species appear at the Katherine Ordway Natural History Study Area in Inver Grove Heights, MN.\n\n### Set Up\n\n```{r}\n# Load the BabyNames data set into our RStudio environment\ndata(\"OrdwayBirds\")\n\n# Inspect the data\nstr(OrdwayBirds)\n```\n\n\n\n### Step 0\n\nBefore we begin, the book gives us some instructions to select a few key variables and clean up the date formatting.  Use the commands given in the book, but be sure to study the functions used so you know what is happening.  In this case, we select `SpeciesName`, `Month`, and `Date`. Then a `mutate()` command converts each variable to character, and then converts the resulting \"character string\" to a number.\n\n```{r}\n# Get the data table & clean up dates (see Data Computing p. 163)\nOrdwayBirds <- \n  OrdwayBirds %>%\n  select(SpeciesName, Month, Day) %>%\n  mutate(Month = as.numeric(as.character(Month)), \n         Day = as.numeric(as.character(Day)))\n\n# Inspect result\nhead(OrdwayBirds)\n```\n\n\n### Step 1\n\nThere are 275 unique \"species names\" in the `OrdwayBirds` data if mis-spellings are counted among the unique entries, and 109 distinct species.\n\n```{r}\n# unique species in original data (including mis-spellings)\nOrdwayBirds %>%\n  summarise(uniqueNamesMisSpelled = n_distinct(SpeciesName))\n\n# unique species in the clean list\nOrdwaySpeciesNames %>%\n  summarise(uniqueSpecies = n_distinct(SpeciesNameCleaned))\n```\n\n\n### Step 2\n  \n* The `inner_join()` uses the `SpeciesName` variable found in both data tables to match cases.  \n* The variables that were added to the `Corrected` table include a clean version of species names called `Species`, as well as the `Month` and `Day` of the bird sightings recorded.\n\n```{r}\n# inspect OrdwaySpeciesNames\nstr(OrdwaySpeciesNames)\n\n# inner join\nCorrected <- \n  OrdwayBirds %>%\n  inner_join(OrdwaySpeciesNames) %>%\n  select(Species = SpeciesNameCleaned, Month, Day) %>%\n  na.omit()  ## clened up missing names\n\n# inspect the data after the join\nhead(Corrected)\n```\n\n\n### Step 3\n\nWe want to identify the top 5 bird species sighted at the Katherine Ordway Natural History Study Area.\n\n```{r}\n# count sightings by species in descending order\nSpeciesCount <- \n  Corrected %>%\n  group_by(Species) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count))\n\n# a hint on p.164 recommends displaying the top 10 species to choose our threshold\nhead(SpeciesCount, 10)\n\n# create a sightings threshold that we will use to define the major species\nthreshold <- 750 \n\nMajors <-\n  SpeciesCount %>%\n  filter(count >= threshold)  # threshold is just a scalar defined above\n\nMajors\n```\n\n\n### Step 4\n\nWe now want to produce a month-by-month count for each of the major species, and then construct a chart to tell the story of what time of year the various species appear.\n\n```{r}\nByMonth <- \n  Majors %>%\n  left_join(Corrected) %>%\n  group_by(Month, Species) %>%\n  summarise(count = n()) %>%\n  arrange(Month, desc(count))\n\n```\n\nNow, using `barGraphHelper()` in the console, we can configure a reasonable graph and choose \"show expression\" to get `ggplot2` that can be embedded in our report.\n\n\n\n```{r}\nggplot(data=ByMonth,aes(x=Month,y=count ,fill=Species)) + geom_bar(stat='identity',position='dodge', width=.9) \n\n```\n\nAccording to the graph, we can answer the questions posed:\n\n##### 1. Which species are present year-round?  \n\nIt appears that the American Goldfinch, and Black-capped Chickadee were sighted every month (though with fluctuation)\n\n##### 2. Which species are migratory, that is, primarily present in one or two seasons?\n\n(solutions vary) It appears that the Field Sparrow, Slate-colored Junco, and Lincoln's Sparrow appear to be primarily present in one or two seasons and are likely migratory.\n\n##### 3. What is the peak month for each major species?\n\n* American Goldfinch: October (10)\n* Black-capped Chickadee: November (11)\n* Field Sparrow: May (5)\n* Lincoln's Sparrow: October (10)\n* Slate-colored Junco: October (10)\n* Tree Swallow: April (4)\n\nIf we set a threshold for \"seen in good numbers\" as 100 sightings in a month based on the graph (i.e. a little over 3 per day), then we might conclude that Field Sparrows, Slate-Colored Juncos, and Tree Swallows are seen \"in good numbers\" for at least 6 months of the year.  A few lines of code can validate what we're seeing visually in the graph:\n\n```{r}\nByMonth %>%\n  filter(count >= 100) %>%\n  group_by(Species) %>%\n  summarise(monthsPresent = n_distinct(Month)) %>%\n  filter(monthsPresent >= 6)\n```\n\n\n\n\n\n\n\n",
    "created" : 1491256809766.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1055756962",
    "id" : "8280EFD4",
    "lastKnownWriteTime" : 1491256869,
    "last_content_update" : 1491256963465,
    "path" : "~/GitHub/job2/index.Rmd",
    "project_path" : "index.Rmd",
    "properties" : {
        "last_setup_crc32" : "C8547D4C9a27d837"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}